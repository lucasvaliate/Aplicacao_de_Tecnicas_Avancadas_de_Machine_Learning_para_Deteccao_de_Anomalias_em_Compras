{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9-InY16CzRL"
      },
      "source": [
        "## Carregar base de Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYXhwGUaCjBZ"
      },
      "outputs": [],
      "source": [
        "# Importação dos pacotes\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "# Seed para reprodução de resultados\n",
        "seed = 1\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYawMFvRM15R"
      },
      "outputs": [],
      "source": [
        "data = pd.read_excel('basetccfinal.xlsx')\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qG8Syhnoxa8E"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igUqkb0WaDQD"
      },
      "outputs": [],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IirKBiyVBFj"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bV2axNZI6KcZ"
      },
      "outputs": [],
      "source": [
        "n = data.nunique(axis=0)\n",
        "print(\"No.of.unique values in each column :\\n\",\n",
        "      n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnsxZ77wa7aq"
      },
      "outputs": [],
      "source": [
        "data2=data.copy()\n",
        "data2=data2.drop(columns=['preco','Total Impostos (ICMS e FCP)','Index','Cod Familia Item','Dsc 1 Item','Business Unit Cd', 'Business Unit Desc','CNPJ / CPF','Cod Comprador Contrato','Cod Comprador Ped Atual','Cod Filial / Fábrica','Depto Comprador Contrato','Dsc 2 Item','Dsc Familia Item','Dsc Filial / Fábrica','Nom Comprador Ped Atual','Nom Comprador Ped Atual','Nome Comprador Contrato','Número Contrato','Servico (S/N)','Subconta','Unidade Medida Compra','Área Suprimentos Atual','Área Suprimentos Original','Área Suprimentos','dolarizar','Dsc Tipo Pedido','Descrição Condição Pagamento','Nome Conta','Tipo Conta','Mes Emissão NF','Mês Criação Pedido','Generico'])\n",
        "data2 = data2.loc[:, ~data2.columns.str.contains('^Unnamed')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DaFZaotLILf"
      },
      "outputs": [],
      "source": [
        "data2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qblf53Z2bS4l"
      },
      "outputs": [],
      "source": [
        "data2.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEl7ghG45dJD"
      },
      "outputs": [],
      "source": [
        "col = data2.select_dtypes(exclude=['number']).columns.tolist()\n",
        "for col in data2.columns:\n",
        "    unique_values = set(data2[col].apply(type))\n",
        "    print(col, unique_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KpBeBCEOg5V"
      },
      "outputs": [],
      "source": [
        "# converter todas as colunas numéricas para float\n",
        "num_cols = data2.select_dtypes(include=[int, float]).columns\n",
        "data2.loc[:, num_cols] = data2.loc[:, num_cols].astype('float').copy()\n",
        "\n",
        "# converter todas as colunas de texto para string\n",
        "text_cols = data2.select_dtypes(include=object).columns\n",
        "data2.loc[:, text_cols] = data2.loc[:, text_cols].astype('object').copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OifG-1-37Oci"
      },
      "outputs": [],
      "source": [
        "\n",
        "for col in data2.columns:\n",
        "    unique_values = set(data2[col].apply(type))\n",
        "    print(col, unique_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKMeAObp00cG"
      },
      "outputs": [],
      "source": [
        "# Identificar as colunas datetime64\n",
        "cols_datetime = data2.select_dtypes(include=['datetime64']).columns.tolist()\n",
        "\n",
        "# Imprimir as colunas identificadas\n",
        "print(cols_datetime)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformação de variáveis categórias em númericas usando LabelEncoder"
      ],
      "metadata": {
        "id": "SU9gHR7dudyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data2['data_fechamento'] = pd.to_datetime(data2['data_fechamento'])"
      ],
      "metadata": {
        "id": "vsTYogss0gA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2['data_fechamento'].head()"
      ],
      "metadata": {
        "id": "x0XUF5UE0hX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from joblib import dump, load\n",
        "\n",
        "# Separar as variáveis numéricas das categóricas\n",
        "Colunas_num = data2.select_dtypes(include=['float', 'int']).columns.tolist()\n",
        "X_numerical = data2[Colunas_num]\n",
        "\n",
        "# Criar um dicionário para armazenar os objetos LabelEncoder\n",
        "label_encoders = {}\n",
        "data_transf= pd.DataFrame()\n",
        "colunas_obj = data2.select_dtypes(include = \"object\").columns\n",
        "\n",
        "# Iterar sobre as colunas categóricas do dataframe\n",
        "for obj in colunas_obj:\n",
        "    # Criar um novo objeto LabelEncoder\n",
        "    le = LabelEncoder()\n",
        "    # Aplicar o LabelEncoder na coluna categórica\n",
        "    data_transf[obj] = le.fit_transform(data2[obj].astype(str))\n",
        "    # Salvar o objeto LabelEncoder em um arquivo\n",
        "    dump(le, f'{obj}_label_encoder.joblib')\n",
        "    # Armazenar o objeto LabelEncoder no dicionário\n",
        "    label_encoders[obj] = le\n",
        "\n",
        "# Converter a coluna datetime64 para segundos desde o Unix Epoch\n",
        "unix_time = data2['data_fechamento'].min()\n",
        "X_date = (data2['data_fechamento'] - unix_time).dt.total_seconds()\n",
        "\n",
        "# Criação do novo dataframe\n",
        "df_tratado = pd.concat([X_numerical, data_transf, X_date], axis=1)\n",
        "df_tratado.head()\n"
      ],
      "metadata": {
        "id": "X0NihCDNuexi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_tratado.isna().sum())"
      ],
      "metadata": {
        "id": "gNOHTJUV8uIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df_tratado.columns:\n",
        "    unique_values = set(df_tratado[col].apply(type))\n",
        "    print(col, unique_values)"
      ],
      "metadata": {
        "id": "8H-ojFhihLPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tratar missing values\n"
      ],
      "metadata": {
        "id": "1Pejjl0gf8ZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import missingno as msno\n",
        "X = df_tratado.drop('cons_preco', axis=1)\n",
        "msno.matrix(X, figsize=(25,5));"
      ],
      "metadata": {
        "id": "GeJaC_BA5spJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_tratado.isna().sum())"
      ],
      "metadata": {
        "id": "gmmKHhjA9vR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Cria um objeto SimpleImputer\n",
        "imputer = SimpleImputer(strategy='most_frequent', fill_value=0)\n",
        "X = df_tratado\n",
        "\n",
        "# Aplica o imputador aos dados de entrada\n",
        "X_tratado = imputer.fit_transform(X)\n",
        "\n",
        "data_SVM = pd.DataFrame(X_tratado, columns=df_tratado.columns)"
      ],
      "metadata": {
        "id": "tNGGeEScgB3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_SVM.isna().sum())"
      ],
      "metadata": {
        "id": "JSGHOUoOCTyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_SVM.head()"
      ],
      "metadata": {
        "id": "sTlgolL1mgkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pré-Processamento para aplicar o PCA"
      ],
      "metadata": {
        "id": "nXHdIwr6uAw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "data_pca=data_SVM\n",
        "data_pca.head()"
      ],
      "metadata": {
        "id": "0bQBKcT1q_OG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_pca.columns)"
      ],
      "metadata": {
        "id": "VtuzpGn4TWJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vamos aplicar o PCA para buscar melhorar o modelo, avaliando o impacto desta etapa nos esultados do modelo.\n",
        "#Pré-processamento PCA (padronização e normalização). \n",
        "#Como o PCA é sensível à escala dos dados, vamos usar o fit_transform para ajustar a escala dos dados com base na distribuição e transformar os dados originais para que possam ser utilizados pelo PCA.\n",
        "# Vamos também utilizar o StandardScaler para padronizar os recursos do conjunto de dados (Média 0 e desvio 1)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "y = data_pca['cons_preco']\n",
        "X_numerical = data_pca.drop('cons_preco', axis=1)\n",
        "\n",
        "# Aplicar normalização nos dados numéricos\n",
        "scaler = StandardScaler()\n",
        "X_normalized = scaler.fit_transform(X_numerical)\n",
        "\n",
        "# Juntar a variável target novamente com os dados normalizados\n",
        "data_normalized = pd.concat([pd.DataFrame(X_normalized , columns=X_numerical.columns), y], axis=1)\n",
        "\n",
        "print(data_normalized.columns)\n",
        "\n"
      ],
      "metadata": {
        "id": "B7P_ldtcEHsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plota as densidades de probabilidade antes e depois da normalização\n",
        "for col in num_cols:\n",
        "    fig, ax = plt.subplots(ncols=2, figsize=(16, 4))\n",
        "\n",
        "    # Plot da densidade de probabilidade antes da normalização\n",
        "    sns.kdeplot(data_pca[col], ax=ax[0])\n",
        "    ax[0].set_title(f\"{col} antes da normalização\")\n",
        "\n",
        "    # Plot da densidade de probabilidade depois da normalização\n",
        "    sns.kdeplot(data_normalized[col], ax=ax[1])\n",
        "    ax[1].set_title(f\"{col} depois da normalização\")\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "nF2W-zi5WgSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_pca2 = data_normalized"
      ],
      "metadata": {
        "id": "AbX70Ei9Ep4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aplicando o PCA"
      ],
      "metadata": {
        "id": "FoEGtkQ0OrFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DIvidindo as bases de treino e de teste\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_rf_pca = data_pca2\n",
        "\n",
        "# Separar os dados em variáveis preditoras (X) e target (y)\n",
        "X = data_rf_pca.drop('cons_preco', axis=1)\n",
        "y = data_rf_pca['cons_preco']\n",
        "\n",
        "# Dividir os dados em conjuntos de treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "_ZgwWWf2cK53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#balanceamento da base de treino\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Cria o objeto da classe RandomOverSampler\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "\n",
        "# Aplica o resampling no conjunto de treino\n",
        "X_train_res, y_train_res = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "# Imprime o número de exemplos em cada conjunto\n",
        "print(\"Número de exemplos no conjunto de treino antes do balanceamento: \", X_train.shape[0])\n",
        "print(\"Número de exemplos no conjunto de treino depois do balanceamento: \", X_train_res.shape[0])\n"
      ],
      "metadata": {
        "id": "u3soI8VDpQhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rodando o PCA\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "# Cria um objeto PCA com n_components=0.95 para manter 95% da variância\n",
        "pca = PCA(n_components=0.95)\n",
        "\n",
        "# Aplica o PCA nos dados X e armazena em X_pca\n",
        "X_pca_res = pca.fit_transform(X_train_res)\n",
        "\n",
        "\n",
        "# Criando o novo dataframe com as novas features geradas pelo PCA\n",
        "data_pca_res = pd.concat([pd.DataFrame(X_pca_res), y_train_res], axis=1)"
      ],
      "metadata": {
        "id": "Bqmj1oxI760I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RandomForest pós PCA sem class weight"
      ],
      "metadata": {
        "id": "PbCyZ2x8T52o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gqmhPtfCUEOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_evaluate(X_test, y_test, model, average='macro'):\n",
        "\n",
        "  # inferência do teste\n",
        "  y_pred = model.predict(X_test) \n",
        "\n",
        "  # Acurácia\n",
        "  from sklearn.metrics import accuracy_score\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  print('Acurácia: ', accuracy)\n",
        "\n",
        "  # Kappa\n",
        "  from sklearn.metrics import cohen_kappa_score\n",
        "  kappa = cohen_kappa_score(y_test, y_pred)\n",
        "  print('Kappa: ', kappa)\n",
        "\n",
        "  # F1\n",
        "  from sklearn.metrics import f1_score\n",
        "  f1 = f1_score(y_test, y_pred, average=average)\n",
        "  print('F1: ', f1)\n",
        "\n",
        "  from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
        "\n",
        "  # Precisão\n",
        "  precision = precision_score(y_test, y_pred, average='macro')\n",
        "  print('Precisão: ', precision)\n",
        "\n",
        "  # Recall\n",
        "  recall = recall_score(y_test, y_pred, average='macro')\n",
        "  print('Recall: ', recall)\n",
        "\n",
        "  # Matriz de confusão\n",
        "  from sklearn.metrics import confusion_matrix\n",
        "  # Gerando a matriz de confusão\n",
        "  confMatrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "  # Normalizando a matriz de confusão\n",
        "  row_sums = confMatrix.sum(axis=1, keepdims=True)\n",
        "  norm_confMatrix = confMatrix / row_sums\n",
        "\n",
        "  ax = plt.subplot()\n",
        "  sns.heatmap(norm_confMatrix, annot=True, fmt=\".2f\", cmap='Blues')\n",
        "  ax.set_xlabel('Previsto')\n",
        "  ax.set_ylabel('Real')\n",
        "  ax.set_title('Matriz de Confusão Normalizada')\n",
        "\n",
        "  # Colocar os nomes\n",
        "  ax.xaxis.set_ticklabels(['0', '1', '2', '3', '4']) \n",
        "  ax.yaxis.set_ticklabels(['0', '1', '2', '3', '4'])\n",
        "  plt.show()\n",
        "\n",
        "  # Retornar as métricas\n",
        "  return accuracy, kappa, f1, norm_confMatrix\n",
        "\n"
      ],
      "metadata": {
        "id": "Znlw9YLfUVZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_RFE=data_pca_res"
      ],
      "metadata": {
        "id": "GYYm_WumVKbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# separar as características e o target\n",
        "X = data_RFE.drop('cons_preco', axis=1)\n",
        "y = data_RFE['cons_preco']\n",
        "\n",
        "# dividir os dados em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# instanciar o modelo\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# treinar o modelo\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# fazer as previsões com o modelo treinado\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# avaliar o modelo\n",
        "print(classification_report(y_test, y_pred))\n",
        "average = 'macro'"
      ],
      "metadata": {
        "id": "gwmZteZUVBBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_and_evaluate(X_test, y_test, rf)"
      ],
      "metadata": {
        "id": "gCKqa03cj9iK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}